{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0902526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries \n",
    "#pip install 'gymnasium[box2d]' pygame matplotlib box2d\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import utils\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83d00b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating env \n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1df21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0429e2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding seed so it always stays same\n",
    "env.action_space.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0201d428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
      "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
      "  1.         1.       ], (8,), float32)\n",
      "Shape of state vector: (8,)\n"
     ]
    }
   ],
   "source": [
    "# Observation space\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Shape of state vector:\", env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21e8f11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example state samples:\n",
      "[-1.9009957  -2.2756608   8.252366   -6.984867    4.2583613   7.250959\n",
      "  0.13593407  0.40009543]\n",
      "[ 0.9890819   0.51520437 -8.563686   -3.2265618  -3.2137537   2.566121\n",
      "  0.33255854  0.83416903]\n",
      "[ 1.0608166  -0.6380764   2.0276923   2.3738945   1.601922    4.4841666\n",
      "  0.22414678  0.38137743]\n"
     ]
    }
   ],
   "source": [
    "# Print example states\n",
    "print(\"\\nExample state samples:\")\n",
    "for _ in range(3):\n",
    "    print(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f44acc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Action space: Discrete(4)\n",
      "Number of discrete actions: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Action space\n",
    "print(\"\\nAction space:\", env.action_space)\n",
    "print(\"Number of discrete actions:\", env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7be450bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All possible actions:\n",
      "Action 0\n",
      "Action 1\n",
      "Action 2\n",
      "Action 3\n"
     ]
    }
   ],
   "source": [
    "# Print all possible actions\n",
    "print(\"\\nAll possible actions:\")\n",
    "for a in range(env.action_space.n):\n",
    "    print(f\"Action {a}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc6408a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State:  (array([-0.00278196,  1.4147277 , -0.28180486,  0.16921836,  0.00323045,\n",
      "        0.06383304,  0.        ,  0.        ], dtype=float32), {})\n",
      "Action Taken:   0\n",
      "Next State:     [-0.00556431  1.4179573  -0.28142354  0.14352864  0.00638476  0.0630924\n",
      "  0.          0.        ]\n",
      "Reward:         0.640480647797915\n",
      "Done:           False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reset the environment and get the initial state.\n",
    "current_state = env.reset()\n",
    "\n",
    "# Select an action\n",
    "action = 0\n",
    "\n",
    "# Run a single time step of the environment's dynamics with the given action.\n",
    "next_state, reward, terminated, truncated, info = env.step(action)\n",
    "done = terminated or truncated\n",
    "\n",
    "print(f\"{'Current State:':<15} {current_state}\")\n",
    "print(f\"{'Action Taken:':<15} {action}\")\n",
    "print(f\"{'Next State:':<15} {next_state}\")\n",
    "print(f\"{'Reward:':<15} {reward}\")\n",
    "print(f\"{'Done:':<15} {done}\")\n",
    "\n",
    "\n",
    "# Replace the `current_state` with the state after the action is taken\n",
    "current_state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179460b",
   "metadata": {},
   "source": [
    "## Implementing Q learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f84caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# records the current time to later compute how long training took.\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c33dd764",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Total number of episodes (training iterations).\n",
    "#Each episode represents a full run from initial state to terminal state (or until the timestep limit).\n",
    "#The agent learns from each episode.\n",
    "num_episodes = 100\n",
    "\n",
    "#Maximum steps allowed per episode.\n",
    "#Prevents infinite loops in non-terminal environments.\n",
    "max_num_timesteps = 50\n",
    "\n",
    "#Stores the cumulative reward (or \"points\") obtained in each episode.\n",
    "total_point_history = []\n",
    "\n",
    "#Used to compute a moving average of the total points over the last 10 episodes.\n",
    "num_p_av = 10    # number of total points to use for averaging\n",
    "\n",
    "#With probability ε, choose a random action (exploration).\n",
    "#With probability 1-ε, choose the best known action (exploitation).\n",
    "epsilon = 1.0     # initial ε value for ε-greedy policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a726e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Initialize memory buffer D with capacity N\n",
    "memory_length = 10000\n",
    "#N = capacity of the buffer, i.e., how many experiences it can store.\n",
    "#deque comes from Python’s collections module: from collections import deque.\n",
    "#It's a data structure like a list but optimized for fast appends and pops from both ends.\n",
    "#Think of it as a queue with automatic memory management when a max length is set.\n",
    "memory_buffer = deque(maxlen=memory_length)\n",
    "\n",
    "\n",
    "#2 Initialize Q-Network with random weights w\n",
    "q_network = Sequential([ \n",
    "    tf.keras.layers.Dense(24, activation='relu',input_shape=(state_size)),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=num_actions, activation='linear') \n",
    "    ])\n",
    "\n",
    "\n",
    "#3 Initialize target Q-Network with weights w = w\n",
    "q_target = Sequential([ \n",
    "    tf.keras.layers.Dense(24, activation='relu',input_shape=(state_size)),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=num_actions, activation='linear') \n",
    "    ])\n",
    "\n",
    "#Making both network weight same \n",
    "q_target.set_weights(q_network.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a533917f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '₁' (U+2081) (3647325864.py, line 20)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m5 Receive initial observation state S ₁\u001b[39m\n                                          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '₁' (U+2081)\n"
     ]
    }
   ],
   "source": [
    "#4 for episode i = 1 to M do\n",
    "for i in range(num_episodes):\n",
    "    #5 Receive initial observation state S ₁\n",
    "    # Reset the environment and get the initial state.\n",
    "    state = env.reset()\n",
    "    total_points = 0 \n",
    "    \n",
    "     #for t=1 to T do Observe state St and choose action At using an e-greedy policy\n",
    "    for t in range(max_num_timesteps):\n",
    "        #creating a state_qn from state dimensions\n",
    "        state_qn = np.expand_dims(state, axis=0)\n",
    "        \n",
    "        '''\n",
    "        Passes the (expanded) state through the Q-network.\n",
    "\n",
    "        The Q-network predicts Q-values for all possible actions from this state.\n",
    "\n",
    "        Output is typically a 1D array: e.g. [Q(s,a₀), Q(s,a₁), ..., Q(s,aₙ)]\n",
    "        '''\n",
    "        q_values = q_network(state_qn)\n",
    "\n",
    "        #Choose an action using ε-greedy strategy.\n",
    "        action = utils.get_action(q_values, epsilon)\n",
    "        \n",
    "        #Take action At in the environment , receive reward Rt and next state St+1\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        #Store experience tuple ( St , At , Rt , St+1 ) in memory buffer D\n",
    "        \n",
    "        \n",
    "10 Every C steps perform a learning update :\n",
    "11 Sample random mini-batch of experience tuples ( Sj , Aj , Rj , Sj+1 ) from D\n",
    "12 Set yj = R ; if episode terminates at step j + 1 , otherwise set yj = R + maxa ' Q(sj+1 , α ' )\n",
    "13 Perform a gradient descent step on ( yjQ(sj , aj ; w))2 with respect to the Q-Network weights w\n",
    "14 Update the weights of the Q-Network using a soft update\n",
    "15 end\n",
    "16 end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
